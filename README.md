# NeuralFusionCore

## Direct Portfolio Weight Forecasting with (News + OHLCV) Crossâ€‘Gated Attention Fusion

This variant directly forecasts **portfolio weights** using multiâ€‘modal inputs (news + OHLCV) and fuses the streams with **Crossâ€‘Gated Attention (CGA)**.  
CGA lets each stream attend to the other via gates that modulate information flow, improving robustness over naive concatenation.

---
## ðŸ“‹ Table of Contents

- [Direct Portfolio Weight Forecasting](#direct-portfolio-weight-forecasting-with-news--ohlcv-cross-gated-attention-fusion)
- [ðŸ—ï¸ Architecture Overview](#-architecture-overview)
  - [Encoders](#encoders)
  - [Fusion â€” Cross-Gated Attention (CGA)](#fusion--cross-gated-attention-cga)
  - [Output Head](#output-head)
- [ðŸŽ¯ Training Objective](#training-objective)
  - [Portfolio Weighting (Top-k Long/Short)](#portfolio-weighting-top-k-longshort)
  - [Sharpe Ratio Loss](#sharpe-ratio-loss-maximize-risk-adjusted-return)
  - [Regularization Terms](#regularization-terms)
  - [Total Loss](#total-loss)
- [ðŸ“ Repository Layout](#-repository-layout-exact)
- [âš™ï¸ Setup](#-setup)
- [ðŸ§© Script Cheat-Sheet](#-script-cheat-sheet)
- [ðŸš€ Pipeline (Direct Weights)](#-pipeline-direct-weights)
- [ðŸ“¦ Dependencies](#dependencies)
- [ðŸ“Š Outputs](#outputs)
- [ðŸ§  Notes](#notes)
- [ðŸ“š Appendix](#appendix)
  - [Upstream Repositories](#upstream-repositories)
  - [Inspiration](#inspiration)
- [ðŸ‘¥ Authors & Citation](#-authors--citation)
- [ðŸ“ž Support](#-support)
---
## ðŸ—ï¸ Architecture Overview

- **Timeframe:** 3â€‘minute bars  
- **Input Window:** 80 timestamps (~4 hours)  
- **Prediction Horizon:** next 80 timestamps (~4 hours)  
- **Assets:** configurable universe

### Encoders

1. **News stream (single LSTM)**  
   - Each article â†’ **BigBird embedding**  
   - **Average embeddings** of all articles per 3-min window  
   - If no news: use learned **[NO_NEWS]** embedding  
   - **Coverage one-hot** (which stocks are mentioned) is concatenated to the news embedding at each timestamp  
   - The sequence is fed to **one LSTM** â†’ produces news sequence embedding  

2. **OHLCV stream (TimesNet)**  
   - A **TimesNetBlock** processes per-asset OHLCV sequences â†’ produces market embedding  

---

### Fusion â€” Crossâ€‘Gated Attention (CGA)

- Let **N** be the news embedding and **M** the market (OHLCV) embedding  
- Compute **crossâ€‘attention** in both directions (Nâ†’M and Mâ†’N)  
- Apply **gates** (sigmoid/tanh) to the attended features before adding residuals:

```math
\tilde{N} = g_N \odot \text{Attn}(N \rightarrow M) + (1-g_N) \odot N
````

```math
\tilde{M} = g_M \odot \text{Attn}(M \rightarrow N) + (1-g_M) \odot M
```

* Concatenate or sum $\tilde{N}$ and $\tilde{M}$ to form the **fused embedding**

---

### Output Head

* A linear layer maps the fused embedding to **portfolio weights** for all assets

---

# Training Objective

The model uses a **top-k long/short portfolio construction** and optimizes a **risk-adjusted return loss with regularization**.

Let:

- $w \in \mathbb{R}^N$ be the portfolio weights computed from logits  
- $R \in \mathbb{R}^{H \times N}$ be the returns matrix for a batch (H time steps, N assets)  
- $k$ be the number of assets to select for active trading  
- $\epsilon$ a small constant for numerical stability  

---
## Portfolio Weighting (Top-k Long/Short)

Weights are computed as:

$$
w = \text{apply}(\text{logits}, k)
$$

where the function `topk_long_short_abs` selects the top-k absolute logits and normalizes them.

Only the top-k largest absolute values of logits are selected, and the weights are normalized:

$$
w_i =
\begin{cases}
\dfrac{\text{sign}(\text{logits}_i) \cdot |\text{logits}_i|}{\sum_{j \in \text{top-k}} |\text{logits}_j|}, & i \in \text{top-k} \\
0, & \text{otherwise}
\end{cases}
$$

---

## Sharpe Ratio Loss (maximize risk-adjusted return)

Portfolio returns:

$$
R_p = \sum_{i=1}^{N} w_i \cdot R_i
$$

Sharpe ratio:

$$
\text{Sharpe} = \frac{\mathbb{E}[R_p]}{\sqrt{\text{Var}(R_p) + \epsilon}}
$$

Sharpe loss:

$$
\mathcal{L}_{\text{Sharpe}} = - \text{Sharpe}
$$

---

## Regularization Terms

1. **Distribution regularizer** (prevents concentration):

$$
\mathcal{L}_{\text{dist}} = \lambda_{\text{div}} \cdot \frac{1}{N} \sum_{i=1}^N w_i^2
$$

2. **Net exposure regularizer** (encourages market-neutral portfolio):

$$
\mathcal{L}_{\text{net}} = \lambda_{\text{net}} \cdot \left(\sum_{i=1}^N w_i \right)^2
$$

3. **Turnover regularizer** (optional, penalizes large changes in weights):

$$
\mathcal{L}_{\text{turnover}} = \lambda_{\text{turnover}} \cdot \sum_{i=1}^N | w_i - w_i^{\text{prev}} |
$$

---

## Total Loss

The total loss optimized:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{Sharpe}} + \mathcal{L}_{\text{dist}} + \mathcal{L}_{\text{net}} + \mathcal{L}_{\text{turnover}}
$$

- Only the turnover regularizer (`L_turnover`) is applied if previous weights `w_prev` are provided.
- $\lambda_{\text{div}}, \lambda_{\text{net}}, \lambda_{\text{turnover}}$ control the regularization strength.
---

## ðŸ“ Repository Layout (exact)
```
NeuralFusionCore/
     â”œâ”€â”€ data/
     â”‚   â”œâ”€â”€ outputs/
     â”‚   â”‚   â””â”€â”€ model_weights.pt        
     â”‚   â””â”€â”€ processed/
     â”‚       â””â”€â”€ show_files.py                   
     â”‚   
     â”œâ”€â”€ lib/
     â”‚   â”œâ”€â”€ backtest.py
     â”‚   â”œâ”€â”€ backtest_weights.py        
     â”‚   â”œâ”€â”€ dataset.py
     â”‚   â”œâ”€â”€ features.py
     â”‚   â”œâ”€â”€ loss_weights.py            
     â”‚   â”œâ”€â”€ market.py
     â”‚   â”œâ”€â”€ model.py
     â”‚   â”œâ”€â”€ news.py
     â”‚   â”œâ”€â”€ redis_utils.py
     â”‚   â”œâ”€â”€ train.py
     â”‚   â””â”€â”€ utils.py
     â”œâ”€â”€_init__.py
     â”œâ”€â”€ README.md
     â”œâ”€â”€ requirements.txt
     â”œâ”€â”€ config.py
     â””â”€â”€ scripts/
          â”œâ”€â”€ data_ingest_service.py
          â”œâ”€â”€ features_service.py
          â”œâ”€â”€ train_service.py
          â”œâ”€â”€ finetune_service.py
          â”œâ”€â”€ prediction_service.py 
          â””â”€â”€ api_service.py

```
> Any folders missing on your machine will be created by the scripts if needed.

---

## âš™ï¸ Setup

```bash

# Clone repository
git clone https://github.com/Novoxpert/NeuralFusionCore.git
cd NeuralFusionCore


# (optional) create a virtual environment
python -m venv .venv

# Linux/macOS:
source .venv/bin/activate

# Windows (PowerShell):
 .\.venv\Scripts\Activate.ps1

# install exact dependencies
pip install -r requirements.txt
```

---
## ðŸ§© Script Cheatâ€‘Sheet

- **`lib/*.py`** â€” internal modules for datasets, models, features, news embeddings,training loops, utilities, and backtesting specialized for direct weights.  
- **`config.py`** â€” central configuration / argument helpers used by the scripts.
- **`scripts/data_ingest_service.py`** â€” fetch OHLCV from ClickHouse and news from Mongo for the given interval, and push results (per-symbol ohlcv DataFrame pickles and news DataFrame) to Redis.

ðŸ”§ Usage examples:
one-shot latest 4h (use scheduler to run every 4h)
```bash
python -m scripts.data_ingest_service --mode latest --hours 4
```
- **`scripts/features_service.py`** â€” Builds features from Redis.

Modes:
  - train:     full rebuild (includes normalizer + meta)
  - finetune:  incremental build (reuse existing normalizer/meta)
  - inference: build features for inference only (produces online_test.parquet)
  - bridge:    build features for ChronobBridge only
  - time:      select data by start_time/end_time for any mode

ðŸ”§ Usage Examples:
```bash
python -m scripts.features_service --mode finetune --latest_hours 24
```
- **`scripts/train_service.py`** â€” Train from scratch on processed/train.parquet and processed/val.parquet
ðŸ”§ Usage Example:
```bash
python -m scripts.train_service --epocha 50 
```
- **`scripts/finetune_service.py`** â€”Fine-tune an existing saved model using the latest features. If validation loss improves, replace saved model and keep previous version with timestamp.

ðŸ”§ Usage Example:
```bash
python -m scripts.finetune_service --epocha 10 --save_best
```
- **`scripts/prediction_service.py`** â€”Scheduled inference: fetch latest data, compute features, infer model, transform logits into portfolio weights, and save predictions to MongoDB and Redis.

ðŸ”§ Usage Example:
```bash
python -m scripts.prediction_service --hours 4 
```
- **`scripts/api_service.py`** â€” create API for Get NeuralFusion weights from Mongodb.
---
## ðŸš€ Pipeline (Direct Weights)

#### 1) run data_ingest_service
#### 2) run features_service
#### 3) run train_service
#### 4) run prediction_service
---

## Dependencies

* Python 3.12+
* PyTorch 2.x
* Hugging Face `transformers` (BigBird)
---

## Outputs

* Predicted weights per timestamp
* Performance metrics:

  * Sharpe ratio
  * Cumulative P&L
  * Max Drawdown
  * Turnover
* Plots: equity curve, rolling Sharpe, weights heatmap

---

## Notes

* **CGA** allows **directional, gated crossâ€‘attention** between news and market signals
* The **distribution loss** helps prevent one-asset collapse
* **Total Loss** combines Sharpe ratio maximization and regularizations

---

## Appendix

### Upstream Repositories

Influential upstream repositories:

* [**BigBird**](https://github.com/google-research/bigbird): A sparse-attention transformer model enabling efficient processing of longer sequences
* [**finBERT**](https://github.com/ProsusAI/finBERT): A pre-trained NLP model fine-tuned for financial sentiment analysis
* [**Time-Series-Library (TSlib)**](https://github.com/thuml/Time-Series-Library): Library providing deep learning-based time series analysis, covering forecasting, anomaly detection, and classification
---
### Inspiration

This work is inspired by the article:

* [**Stock Movement Prediction with Multimodal Stable Fusion via Gated Cross-Attention Mechanism**](https://arxiv.org/abs/2406.06594): Introduces the Multimodal Stable Fusion with Gated Cross-Attention (MSGCA) architecture, designed to robustly integrate multimodal inputs for stock movement prediction.
---
## ðŸ‘¥ Authors & Citation

**Developed by the [Novoxpert Research Team](https://github.com/Novoxpert)**  
Lead Contributors:
 - [Elham Esmaeilnia](https://github.com/Elham-Esmaeilnia), [Hamidreza Naeini](https://github.com/)
 

If you use this repository or build upon our work, please cite:

> Novoxpert Research (2025). *NeuralFusionCore: Direct Portfolio Weight Forecasting with Cross-Gated Attention Fusion.*  
> GitHub: [https://github.com/Novoxpert/NeuralFusionCore](https://github.com/Novoxpert/NeuralFusionCore)

```bibtex
@software{novoxpert_neuralfusioncore_2025,
  author       = {Elham Esmaeilnia and Hamidreza Naeini},
  title        = {NeuralFusionCore: Direct Portfolio Weight Forecasting with Cross-Gated Attention Fusion},
  organization = {Novoxpert Research},
  year         = {2025},
  url          = {https://github.com/Novoxpert/NeuralFusionCore}
}
```
---
## ðŸ“ž Support

- **Issues & Bugs**: [Open on GitHub](https://github.com/Novoxpert/neuralfusioncore/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Novoxpert/neuralfusioncore/discussions)
- **Feature Requests**: Open a feature request issue
---